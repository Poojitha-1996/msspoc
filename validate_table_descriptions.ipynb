{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a18acdd2-430b-4d5d-91c8-899ec2f6d472",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "import os\n",
    "import sys\n",
    "import base64\n",
    "import requests\n",
    "from pyspark.sql.utils import AnalysisException\n",
    "\n",
    "# Step 1: Get modified files from GitHub Actions\n",
    "dbutils.widgets.text(\"modified_files\", \"[]\")  # Default to empty list\n",
    "modified_files = json.loads(dbutils.widgets.get(\"modified_files\"))\n",
    "\n",
    "if not modified_files:\n",
    "    print(\"‚úÖ No modified files to validate.\")\n",
    "    sys.exit(0)  # Exit successfully if there are no changes\n",
    "\n",
    "# Step 2: Get the Databricks username\n",
    "try:\n",
    "    username = spark.sql(\"SELECT current_user()\").collect()[0][0]\n",
    "except AnalysisException:\n",
    "    print(\"‚ùå Error: Unable to fetch username.\")\n",
    "    sys.exit(1)  # Fail the job\n",
    "\n",
    "# Step 3: Get the repo name dynamically\n",
    "repo_path = dbutils.notebook.entry_point.getDbutils().notebook().getContext().notebookPath().get()\n",
    "repo_name = repo_path.split(\"/\")[3]  # Extract repo name\n",
    "\n",
    "# Step 4: Construct full paths to modified notebooks\n",
    "full_paths = [f\"/Workspace/Users/{username}/{repo_name}/{os.path.splitext(file)[0]}\" for file in modified_files]\n",
    "print(\"üîπ Full Paths to Validate:\", full_paths)\n",
    "\n",
    "# Step 5: Databricks API details\n",
    "DATABRICKS_HOST = \"https://adb-845055060386182.2.azuredatabricks.net\"\n",
    "DATABRICKS_TOKEN = \"dapic77f368dd1174a60661f559bc3f41d65-2\"\n",
    "\n",
    "headers = {\"Authorization\": f\"Bearer {DATABRICKS_TOKEN}\"}\n",
    "\n",
    "# Step 6: Function to fetch and decode notebook content\n",
    "def fetch_notebook_content(notebook_path):\n",
    "    response = requests.get(\n",
    "        f\"{DATABRICKS_HOST}/api/2.0/workspace/export\",\n",
    "        headers=headers,\n",
    "        params={\"path\": notebook_path, \"format\": \"SOURCE\"}  # Fetch raw SQL content\n",
    "    )\n",
    "    if response.status_code == 200:\n",
    "        encoded_content = response.json().get(\"content\", \"\")\n",
    "        return base64.b64decode(encoded_content).decode(\"utf-8\")  # Decode Base64\n",
    "    else:\n",
    "        print(f\"‚ùå Error fetching notebook content ({notebook_path}): {response.text}\")\n",
    "        return None  # Return None on failure\n",
    "\n",
    "# Step 7: Function to validate SQL scripts\n",
    "def validate_sql_script(sql_content):\n",
    "    \"\"\"\n",
    "    Validates if all columns in a CREATE TABLE statement have comments.\n",
    "    Returns True if valid, False otherwise.\n",
    "    \"\"\"\n",
    "    create_table_pattern = re.compile(\n",
    "        r\"CREATE\\s+TABLE\\s+[\\w.]+\\s*\\((.*?)\\)\\s*COMMENT\\s*=\", \n",
    "        re.IGNORECASE | re.DOTALL\n",
    "    )\n",
    "    column_pattern = re.compile(\n",
    "        r\"\\s*(\\w+)\\s+\\w+(\\s+COMMENT\\s+'([^']+)')?\", \n",
    "        re.IGNORECASE\n",
    "    )\n",
    "\n",
    "    match = create_table_pattern.search(sql_content)\n",
    "    if not match:\n",
    "        print(\"‚ö†Ô∏è No CREATE TABLE statement found in this file.\")\n",
    "        return True  # Ignore files that don't create tables\n",
    "\n",
    "    column_definitions = match.group(1)\n",
    "    columns = column_pattern.findall(column_definitions)\n",
    "\n",
    "    missing_comments = [col[0] for col in columns if not col[2]]\n",
    "    if missing_comments:\n",
    "        print(f\"‚ùå Missing comments for columns: {missing_comments}\")\n",
    "        return False  # Fail validation\n",
    "    return True\n",
    "\n",
    "# Step 8: Validate each modified file\n",
    "validation_failed = False\n",
    "\n",
    "for file_path in full_paths:\n",
    "    sql_content = fetch_notebook_content(file_path)\n",
    "    \n",
    "    if sql_content is None:\n",
    "        validation_failed = True  # Fail if notebook fetch fails\n",
    "        continue\n",
    "\n",
    "    if not validate_sql_script(sql_content):\n",
    "        validation_failed = True  # Fail if SQL validation fails\n",
    "\n",
    "# Step 9: Exit with failure if validation fails\n",
    "if validation_failed:\n",
    "    print(\"‚ùå Validation failed! Some tables have missing column comments.\")\n",
    "    sys.exit(1)  # Fail the Databricks job (which fails GitHub Actions)\n",
    "else:\n",
    "    print(\"‚úÖ Validation passed! All tables have proper column comments.\")\n",
    "    sys.exit(0)  # Pass the job\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "validate_table_descriptions",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

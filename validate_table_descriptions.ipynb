{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a18acdd2-430b-4d5d-91c8-899ec2f6d472",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "import sys\n",
    "from pyspark.sql.utils import AnalysisException\n",
    "\n",
    "# Step 1: Get modified files from GitHub Actions\n",
    "dbutils.widgets.text(\"modified_files\", \"[]\")  # Default to empty list\n",
    "modified_files = json.loads(dbutils.widgets.get(\"modified_files\"))\n",
    "\n",
    "if not modified_files:\n",
    "    print(\"‚úÖ No modified files to validate.\")\n",
    "    sys.exit(0)  # Exit successfully if there are no changes\n",
    "\n",
    "# Step 2: Get the Databricks username\n",
    "try:\n",
    "    username = spark.sql(\"SELECT current_user()\").collect()[0][0]\n",
    "except AnalysisException:\n",
    "    print(\"‚ùå Error: Unable to fetch username.\")\n",
    "    sys.exit(1)  # Fail the job\n",
    "\n",
    "# Step 3: Get the repo name dynamically\n",
    "repo_path = dbutils.notebook.entry_point.getDbutils().notebook().getContext().notebookPath().get()\n",
    "repo_name = repo_path.split(\"/\")[3]  # Extract repo name\n",
    "\n",
    "# Step 4: Remove .ipynb extension and construct full paths\n",
    "modified_files = [file.replace(\".ipynb\", \"\") for file in modified_files]  # Remove .ipynb extension\n",
    "full_paths = [f\"/Workspace/Users/{username}/{repo_name}/{file}\" for file in modified_files]\n",
    "print(\"üîπ Full Paths to Validate:\", full_paths)\n",
    "\n",
    "# Step 5: Function to validate SQL scripts\n",
    "def validate_sql_script(sql_content):\n",
    "    \"\"\"\n",
    "    Validates if all columns in a CREATE TABLE statement have comments.\n",
    "    Returns True if valid, False otherwise.\n",
    "    \"\"\"\n",
    "    create_table_pattern = re.compile(r\"CREATE\\s+TABLE\\s+[\\w.]+\\s*\\((.*?)\\)\", re.IGNORECASE | re.DOTALL)\n",
    "    column_pattern = re.compile(r\"(\\w+)\\s+\\w+(\\s+COMMENT\\s+'[^']+')?\", re.IGNORECASE)\n",
    "\n",
    "    match = create_table_pattern.search(sql_content)\n",
    "    if not match:\n",
    "        print(\"‚ö†Ô∏è No CREATE TABLE statement found in this file.\")\n",
    "        return True  # Ignore files that don't create tables\n",
    "\n",
    "    column_definitions = match.group(1)\n",
    "    columns = column_pattern.findall(column_definitions)\n",
    "\n",
    "    missing_comments = [col[0] for col in columns if not col[1]]\n",
    "    if missing_comments:\n",
    "        print(f\"‚ùå Missing comments for columns: {missing_comments}\")\n",
    "        return False  # Fail validation\n",
    "    return True\n",
    "\n",
    "# Step 6: Validate each modified file\n",
    "validation_failed = False\n",
    "\n",
    "for file_path in full_paths:\n",
    "    try:\n",
    "        # Convert workspace path to DBFS path\n",
    "        dbfs_path = f\"dbfs:/mnt/workspace{file_path.replace('/Workspace', '')}\"\n",
    "\n",
    "        # Read the notebook content using dbutils\n",
    "        sql_content = dbutils.fs.head(dbfs_path, 100000)  # Read first 100 KB\n",
    "        \n",
    "        if not validate_sql_script(sql_content):\n",
    "            validation_failed = True\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error reading file {file_path}: {e}\")\n",
    "        validation_failed = True  # Fail validation if file cannot be read\n",
    "\n",
    "# Step 7: Exit with failure if validation fails\n",
    "if validation_failed:\n",
    "    print(\"‚ùå Validation failed! Some tables have missing column comments.\")\n",
    "    sys.exit(1)  # Fail the Databricks job (which fails GitHub Actions)\n",
    "else:\n",
    "    print(\"‚úÖ Validation passed! All tables have proper column comments.\")\n",
    "    sys.exit(0)  # Pass the job\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "validate_table_descriptions",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
